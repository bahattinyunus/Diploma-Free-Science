# ğŸ¤– Proje: Yerel LLM AjanÄ± (Local LLM Agent)

**Zorluk**: ğŸŸ¡ Orta
**Alan**: Yapay Zeka / Veri Bilimi
**Tahmini SÃ¼re**: 2 Hafta Sonu

## ğŸ¯ AmaÃ§
OpenAI veya Google'a veri gÃ¶ndermeden, tamamen kendi bilgisayarÄ±nÄ±zda Ã§alÄ±ÅŸan, sizin pdf/markdown notlarÄ±nÄ±zla konuÅŸabilen (RAG - Retrieval Augmented Generation) Ã¶zel bir yapay zeka asistanÄ± kurmak. "Gizlilik" odaklÄ± bir araÅŸtÄ±rma asistanÄ±.

## ğŸ§° Gereksinimler
1.  **DonanÄ±m**: En az 8GB RAM (16GB Ã¶nerilir). NVIDIA GPU (4GB+ VRAM) bÃ¼yÃ¼k hÄ±z katar ama CPU ile de (yavaÅŸ) Ã§alÄ±ÅŸÄ±r.
2.  **YazÄ±lÄ±m**: Python, Docker (opsiyonel), Ollama.

## ğŸ› ï¸ AdÄ±mlar

### 1. Ollama Kurulumu
`Ollama`, yerel modelleri Ã§alÄ±ÅŸtÄ±rmanÄ±n en kolay yoludur.
-   [Ollama.ai](https://ollama.ai) adresinden indirin.
-   Terminalde `ollama run llama3` (veya `mistral`) yazarak modeli indirin ve konuÅŸmaya baÅŸlayÄ±n.

### 2. DokÃ¼man HazÄ±rlÄ±ÄŸÄ±
-   Okumak istediÄŸiniz PDF makaleleri veya Obsidian notlarÄ±nÄ±zÄ± bir klasÃ¶re toplayÄ±n.
-   Python ile bu metinleri okuyup "kÃ¼Ã§Ã¼k parÃ§alara" (chunks) bÃ¶lÃ¼n.

### 3. VektÃ¶r VeritabanÄ± (Embeddings)
-   `Chromadb` veya `FAISS` kullanarak metin parÃ§alarÄ±nÄ± vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼n ve kaydedin. (Burada `sentence-transformers` kÃ¼tÃ¼phanesi kullanÄ±lÄ±r).

### 4. RAG Boru HattÄ± (Pipeline)
-   KullanÄ±cÄ± sorusunu vektÃ¶re Ã§evir -> VeritabanÄ±nda en yakÄ±n metin parÃ§alarÄ±nÄ± bul -> Bu parÃ§alarÄ± LLM'e "baÄŸlam" (context) olarak ver -> CevabÄ± Ã¼ret.
-   `LangChain` veya basit Python scriptleri ile bu akÄ±ÅŸÄ± kodlayÄ±n.

### 5. ArayÃ¼z (UI)
-   `Streamlit` veya `Chainlit` kullanarak tarayÄ±cÄ± Ã¼zerinden Ã§alÄ±ÅŸan ÅŸÄ±k bir sohbet arayÃ¼zÃ¼ yapÄ±n.

## ğŸš€ Meydan Okuma
-   Ajana "internet eriÅŸimi" verin (Google Search API kullanarak gÃ¼ncel olaylarÄ± araÅŸtÄ±rabilsin).
-   Ajana "kod Ã§alÄ±ÅŸtÄ±rma" yeteneÄŸi verin (Birlikte Python scripti yazÄ±p debug edin).
